# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bqD17R1MgtlAyd0DEvjRLEDvpkEE7WZp
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install faiss-cpu

import streamlit as st
import pickle
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import pandas as pd

# ============================
# Load Data & Models
# ============================
@st.cache_resource
def load_resources():
    df_jobs = pickle.load(open("nco_df.pkl", "rb"))
    df_schemes = pickle.load(open("scheme_df.pkl", "rb"))
    job_embeddings = np.load("nco_embeddings.npy")
    scheme_embeddings = np.load("scheme_embeddings.npy")
    index = faiss.read_index("nco_faiss.index")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    return df_jobs, df_schemes, job_embeddings, scheme_embeddings, index, model

df_jobs, df_schemes, job_embeddings, scheme_embeddings, index, model = load_resources()

# ============================
# Search Functions
# ============================
def semantic_search_jobs(query, top_k=5):
    query_emb = model.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(query_emb)
    D, I = index.search(query_emb, top_k)
    results = df_jobs.iloc[I[0]].copy()
    results['similarity'] = D[0]
    return results

def recommend_schemes(job_text, top_k=3):
    job_emb = model.encode([job_text], convert_to_numpy=True)[0]
    scores = np.dot(scheme_embeddings, job_emb) / (
        np.linalg.norm(scheme_embeddings, axis=1) * np.linalg.norm(job_emb)
    )
    top_idx = np.argsort(scores)[::-1][:top_k]
    results = df_schemes.iloc[top_idx].copy()
    results['similarity'] = scores[top_idx]
    return results

# ============================
# Streamlit UI
# ============================
st.set_page_config(page_title="NCO Job & Scheme Finder", layout="wide")
st.title("üîç NCO Job Search + Govt Scheme Recommender")
st.write("Enter a job title or skill to find matching occupations and government schemes.")

query = st.text_input("Enter Job/Skill:", placeholder="e.g., Solar Technician, Data Entry Operator")
top_k_jobs = st.slider("Number of job matches to display:", 1, 10, 5)

if st.button("Search") and query:
    job_results = semantic_search_jobs(query, top_k=top_k_jobs)

    all_results = []
    for _, job in job_results.iterrows():
        st.subheader(f"[{job['nco_code']}] {job['title']} ‚Äî Score: {job['similarity']:.2f}")
        st.write(job['description'])

        schemes = recommend_schemes(job['text'], top_k=3)
        st.markdown("**Recommended Schemes:**")
        for _, sch in schemes.iterrows():
            st.write(f"- {sch['scheme_name']} ‚Äî Score: {sch['similarity']:.2f}")

        # Store for CSV download
        for _, sch in schemes.iterrows():
            all_results.append({
                "Query": query,
                "NCO Code": job['nco_code'],
                "Job Title": job['title'],
                "Job Description": job['description'],
                "Job Similarity Score": job['similarity'],
                "Scheme Name": sch['scheme_name'],
                "Scheme Description": sch['description'],
                "Scheme Similarity Score": sch['similarity']
            })

    # Download button
    if all_results:
        results_df = pd.DataFrame(all_results)
        csv_data = results_df.to_csv(index=False).encode('utf-8')
        st.download_button("üì• Download Results as CSV", csv_data, "search_results.csv", "text/csv")